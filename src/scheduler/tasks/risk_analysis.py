"""
ê°„ë‹¨í•œ ë¦¬ìŠ¤í¬ ë¶„ì„ ìŠ¤ì¼€ì¤„ëŸ¬
1ì‹œê°„ë§ˆë‹¤ ìƒìœ„ 20ê°œ ì½”ì¸ì— ëŒ€í•œ ë¦¬ìŠ¤í¬ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
"""

import asyncio
import json
import time
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any

from celery import Celery
import ccxt

from src.common.utils.logger import set_logger
from src.config.database import database_config
from src.app.autotrading_v2.risk_service import RiskAnalysisService
import ccxt
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

logger = set_logger(__name__)

# Celery ì•± ì¸ìŠ¤í„´ìŠ¤ (celery.pyì—ì„œ ê°€ì ¸ì˜´)
from src.scheduler.celery import app as celery_app

def get_top_20_coins():
    """ccxtë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒìœ„ 20ê°œ ì½”ì¸ ì¡°íšŒ"""
    try:
        exchange = ccxt.binance()
        markets = exchange.load_markets()

        # USDT í˜ì–´ë§Œ í•„í„°ë§í•˜ê³  ê±°ë˜ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        usdt_pairs = []
        for symbol, market in markets.items():
            if market['quote'] == 'USDT' and market['active']:
                try:
                    ticker = exchange.fetch_ticker(symbol)
                    if ticker['quoteVolume'] and ticker['quoteVolume'] > 0:
                        usdt_pairs.append({
                            'symbol': symbol,
                            'volume': ticker['quoteVolume']
                        })
                except:
                    continue

        # ê±°ë˜ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 20ê°œ ì„ íƒ
        usdt_pairs.sort(key=lambda x: x['volume'], reverse=True)
        top_20 = [pair['symbol'] for pair in usdt_pairs[:20]]

        logger.info(f"âœ… ìƒìœ„ 20ê°œ ì½”ì¸ ì¡°íšŒ ì™„ë£Œ: {len(top_20)}ê°œ")
        return top_20
    except Exception as e:
        logger.error(f"âŒ ìƒìœ„ ì½”ì¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return [
            "BTC/USDT", "ETH/USDT", "BNB/USDT", "XRP/USDT", "ADA/USDT",
            "SOL/USDT", "DOGE/USDT", "DOT/USDT", "AVAX/USDT", "MATIC/USDT",
            "LINK/USDT", "UNI/USDT", "LTC/USDT", "ATOM/USDT", "XLM/USDT",
            "BCH/USDT", "FIL/USDT", "TRX/USDT", "ETC/USDT", "ALGO/USDT"
        ]

async def analyze_individual_coin_risk(coin_symbol: str, risk_service: RiskAnalysisService, days_back: int = 90) -> Dict[str, Any]:
    """
    ê°œë³„ ì½”ì¸ë³„ ê³ ìœ  ë¦¬ìŠ¤í¬ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë¶„ì„
    """
    try:
        # 1. ê¸€ë¡œë²Œ ì‹œì¥ ë¦¬ìŠ¤í¬ ë¶„ì„ (ê¸°ì¡´)
        global_risk = await risk_service.analyze_risk(
            market=coin_symbol,
            analysis_type="daily",
            days_back=days_back,
            personality="neutral",
            include_analysis=False  # AI ë¶„ì„ ë¹„í™œì„±í™”
        )

        # 2. ê°œë³„ ì½”ì¸ ë³€ë™ì„± ë¶„ì„
        coin_volatility = await analyze_coin_volatility(coin_symbol, days_back)

        # 3. ì½”ì¸ë³„ ë¦¬ìŠ¤í¬ ë“±ê¸‰ ì¡°ì •
        adjusted_risk = adjust_risk_for_coin(global_risk, coin_volatility, coin_symbol)

        return adjusted_risk

    except Exception as e:
        logger.error(f"âŒ {coin_symbol} ê°œë³„ ë¦¬ìŠ¤í¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬ ë°˜í™˜
        return await risk_service.analyze_risk(
            market=coin_symbol,
            analysis_type="daily",
            days_back=days_back,
            personality="neutral",
            include_analysis=False
        )

async def analyze_coin_volatility(coin_symbol: str, days_back: int) -> Dict[str, float]:
    """
    ê°œë³„ ì½”ì¸ì˜ ë³€ë™ì„± íŠ¹ì„± ë¶„ì„
    """
    try:
        exchange = ccxt.binance()

        # ìµœê·¼ ë°ì´í„° ìˆ˜ì§‘
        since = int((datetime.now() - timedelta(days=days_back)).timestamp() * 1000)
        ohlcv = exchange.fetch_ohlcv(coin_symbol, '1d', since=since, limit=days_back)

        if len(ohlcv) < 30:  # ìµœì†Œ 30ì¼ ë°ì´í„° í•„ìš”
            return {"volatility_7d": 0.0, "volatility_30d": 0.0, "max_drawdown": 0.0, "sharpe_ratio": 0.0}

        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['returns'] = df['close'].pct_change().dropna()

        # 7ì¼ ë³€ë™ì„±
        volatility_7d = df['returns'].tail(7).std() * np.sqrt(365) * 100

        # 30ì¼ ë³€ë™ì„±
        volatility_30d = df['returns'].tail(30).std() * np.sqrt(365) * 100

        # ìµœëŒ€ ë‚™í­ (Max Drawdown)
        cumulative = (1 + df['returns']).cumprod()
        running_max = cumulative.expanding().max()
        drawdown = (cumulative - running_max) / running_max
        max_drawdown = abs(drawdown.min()) * 100

        # ìƒ¤í”„ ë¹„ìœ¨ (ì—°ê°„í™”)
        sharpe_ratio = (df['returns'].mean() * 365) / (df['returns'].std() * np.sqrt(365)) if df['returns'].std() > 0 else 0

        return {
            "volatility_7d": float(volatility_7d),
            "volatility_30d": float(volatility_30d),
            "max_drawdown": float(max_drawdown),
            "sharpe_ratio": float(sharpe_ratio)
        }

    except Exception as e:
        logger.error(f"âŒ {coin_symbol} ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        return {"volatility_7d": 0.0, "volatility_30d": 0.0, "max_drawdown": 0.0, "sharpe_ratio": 0.0}

def adjust_risk_for_coin(global_risk: Dict[str, Any], coin_volatility: Dict[str, float], coin_symbol: str) -> Dict[str, Any]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ìŠ¤í¬ ë¶„ì„: ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬ + ê°œë³„ ì½”ì¸ íŠ¹ì„±
    """
    try:
        # 1. ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬ ë² ì´ìŠ¤ë¼ì¸
        base_risk_score = global_risk.get('analysis', {}).get('risk_indicators', {}).get('overall_risk_score', 50.0)
        global_risk_level = global_risk.get('risk_grade', 'LOW')

        # 2. ê°œë³„ ì½”ì¸ ë³€ë™ì„± íŠ¹ì„±
        volatility_30d = coin_volatility.get('volatility_30d', 0.0)
        max_drawdown = coin_volatility.get('max_drawdown', 0.0)
        sharpe_ratio = coin_volatility.get('sharpe_ratio', 0.0)

        # 3. ì½”ì¸ë³„ ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜ (ëŒ€ì¥ì£¼ vs ì•ŒíŠ¸ì½”ì¸)
        coin_weight = get_coin_risk_weight(coin_symbol)

        # 4. ë³€ë™ì„± ê¸°ë°˜ ì¡°ì • ê³„ìˆ˜ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
        volatility_factor = min(1.0 + (volatility_30d - 30.0) / 100.0, 2.0)  # 30% ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •
        drawdown_factor = min(1.0 + max_drawdown / 50.0, 2.0)  # 50% ë‚™í­ ê¸°ì¤€
        sharpe_factor = max(0.7, min(1.3, 1.0 - sharpe_ratio / 10.0))  # ìƒ¤í”„ ë¹„ìœ¨ ë³´ìˆ˜ì  ë°˜ì˜

        # 5. ìµœì¢… ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚° (ê¸€ë¡œë²Œ ë² ì´ìŠ¤ + ì½”ì¸ íŠ¹ì„±)
        adjusted_risk_score = base_risk_score * volatility_factor * drawdown_factor * sharpe_factor * coin_weight

        # 6. ë¦¬ìŠ¤í¬ ë“±ê¸‰ ê²°ì • (ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬ë¥¼ ê³ ë ¤í•œ ë²”ìœ„ ë‚´ì—ì„œ)
        if global_risk_level == "LOW":
            if adjusted_risk_score < 25:
                risk_level = "LOW"
            elif adjusted_risk_score < 45:
                risk_level = "MEDIUM"
            else:
                risk_level = "HIGH"
        elif global_risk_level == "MEDIUM":
            if adjusted_risk_score < 35:
                risk_level = "LOW"
            elif adjusted_risk_score < 55:
                risk_level = "MEDIUM"
            else:
                risk_level = "HIGH"
        else:  # HIGH or CRITICAL
            if adjusted_risk_score < 45:
                risk_level = "MEDIUM"
            elif adjusted_risk_score < 65:
                risk_level = "HIGH"
            else:
                risk_level = "CRITICAL"

        # 7. ì‹ ë¢°ë„ ê³„ì‚° (ë°ì´í„° í’ˆì§ˆ + ê¸€ë¡œë²Œ ì¼ê´€ì„±)
        base_confidence = global_risk.get('analysis', {}).get('confidence', 0.6)
        volatility_confidence = 0.8 if volatility_30d > 0 else 0.3
        final_confidence = (base_confidence + volatility_confidence) / 2

        # 8. ê²°ê³¼ êµ¬ì„± (í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°)
        result = global_risk.copy()

        # ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬ ì •ë³´ ìœ ì§€
        result['global_risk'] = {
            'level': global_risk_level,
            'score': base_risk_score,
            'indicators': global_risk.get('analysis', {}).get('risk_indicators', {})
        }

        # ê°œë³„ ì½”ì¸ ë¦¬ìŠ¤í¬ ì •ë³´
        result['individual_risk'] = {
            'level': risk_level,
            'score': adjusted_risk_score,
            'volatility_30d': volatility_30d,
            'max_drawdown': max_drawdown,
            'sharpe_ratio': sharpe_ratio,
            'coin_weight': coin_weight,
            'adjustment_factors': {
                'volatility_factor': volatility_factor,
                'drawdown_factor': drawdown_factor,
                'sharpe_factor': sharpe_factor
            }
        }

        # ê¸°ì¡´ êµ¬ì¡°ì™€ í˜¸í™˜ì„± ìœ ì§€
        result['analysis']['risk_indicators']['overall_risk_score'] = adjusted_risk_score
        result['analysis']['risk_indicators']['coin_volatility_30d'] = volatility_30d
        result['analysis']['risk_indicators']['coin_max_drawdown'] = max_drawdown
        result['analysis']['risk_indicators']['coin_sharpe_ratio'] = sharpe_ratio
        result['analysis']['risk_indicators']['coin_risk_factor'] = coin_weight
        result['risk_grade'] = risk_level
        result['analysis']['confidence'] = final_confidence

        logger.info(f"ğŸ“Š {coin_symbol} í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ìŠ¤í¬: ê¸€ë¡œë²Œ({global_risk_level}:{base_risk_score:.1f}) + ê°œë³„({risk_level}:{adjusted_risk_score:.1f})")

        return result

    except Exception as e:
        logger.error(f"âŒ {coin_symbol} í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ìŠ¤í¬ ì¡°ì • ì‹¤íŒ¨: {str(e)}")
        return global_risk

def get_coin_risk_weight(coin_symbol: str) -> float:
    """
    ì½”ì¸ë³„ ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜ (ëŒ€ì¥ì£¼ vs ì•ŒíŠ¸ì½”ì¸)
    """
    # ëŒ€ì¥ì£¼ (ë‚®ì€ ë¦¬ìŠ¤í¬)
    major_coins = ["BTC/USDT", "ETH/USDT", "BNB/USDT", "XRP/USDT", "ADA/USDT"]

    # ì¤‘ê°„ ê·œëª¨ ì½”ì¸
    mid_caps = ["SOL/USDT", "DOT/USDT", "AVAX/USDT", "MATIC/USDT", "LINK/USDT", "UNI/USDT"]

    if coin_symbol in major_coins:
        return 0.8  # 20% ë¦¬ìŠ¤í¬ ê°ì†Œ
    elif coin_symbol in mid_caps:
        return 1.0  # ê¸°ë³¸ ë¦¬ìŠ¤í¬
    else:
        return 1.3  # 30% ë¦¬ìŠ¤í¬ ì¦ê°€ (ì•ŒíŠ¸ì½”ì¸)

async def save_risk_analysis_to_database(analysis_data: Dict[str, Any]):
    """ë¦¬ìŠ¤í¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    import asyncpg

    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        conn = await asyncpg.connect(
            host=database_config.POSTGRESQL_DB_HOST,
            port=int(database_config.POSTGRESQL_DB_PORT),
            database=database_config.POSTGRESQL_DB_DATABASE,
            user=database_config.POSTGRESQL_DB_USER,
            password=database_config.POSTGRESQL_DB_PASSWORD
        )

        try:
            # ê¸°ì¡´ ë¦¬ìŠ¤í¬ ë¶„ì„ ì„œë¹„ìŠ¤ ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ ë°ì´í„° ì¶”ì¶œ
            asset_symbol = analysis_data.get('market', '')
            risk_grade = analysis_data.get('risk_grade', 'unknown')
            analysis = analysis_data.get('analysis', {})

            # analysisì—ì„œ ì„¸ë¶€ ë°ì´í„° ì¶”ì¶œ
            risk_indicators = analysis.get('risk_indicators', {})
            correlation_analysis = analysis.get('correlation_analysis', {})
            risk_off_signal = analysis.get('risk_off_signal', False)
            confidence = analysis.get('confidence', 0.0)

            # risk_scoreëŠ” risk_indicatorsì—ì„œ ê³„ì‚°í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
            risk_score = 0.0
            if isinstance(risk_indicators, dict) and 'overall_risk_score' in risk_indicators:
                risk_score = risk_indicators['overall_risk_score']

            # JSON ì§ë ¬í™”
            risk_indicators_json = json.dumps(risk_indicators) if risk_indicators else '{}'
            correlation_analysis_json = json.dumps(correlation_analysis) if correlation_analysis else '{}'
            full_analysis_data_json = json.dumps(analysis_data)

            # ë§Œë£Œ ì‹œê°„ (1ì‹œê°„ í›„)
            expires_at = datetime.now(timezone.utc) + timedelta(hours=1)

            # INSERT ì¿¼ë¦¬ (ê¸°ì¡´ ë°ì´í„°ì™€ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë‹¨ìˆœ INSERT)
            query = """
                INSERT INTO risk_analysis_reports
                (asset_symbol, risk_score, market_risk_level, risk_off_signal, confidence,
                 risk_indicators, correlation_analysis, full_analysis_data, expires_at)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
            """

            await conn.execute(
                query,
                asset_symbol, risk_score, risk_grade, risk_off_signal, confidence,
                risk_indicators_json, correlation_analysis_json,
                full_analysis_data_json, expires_at
            )

            logger.info(f"âœ… ë¦¬ìŠ¤í¬ ë¶„ì„ ì €ì¥ ì™„ë£Œ: {asset_symbol} | {risk_grade} | Risk-Off: {risk_off_signal}")

        finally:
            await conn.close()

    except Exception as e:
        logger.error(f"âŒ ë¦¬ìŠ¤í¬ ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        raise

@celery_app.task(bind=True, name='scheduler.tasks.risk_analysis.analyze_top_20_risk')
def analyze_top_20_risk(self):
    """ìƒìœ„ 20ê°œ ì½”ì¸ì— ëŒ€í•œ ë¦¬ìŠ¤í¬ ë¶„ì„ ì‹¤í–‰"""
    try:
        logger.info("ğŸš€ ë¦¬ìŠ¤í¬ ë¶„ì„ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")

        # ìƒìœ„ 20ê°œ ì½”ì¸ ì¡°íšŒ
        top_coins = get_top_20_coins()
        logger.info(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ì½”ì¸: {len(top_coins)}ê°œ")

        # ë¦¬ìŠ¤í¬ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        risk_service = RiskAnalysisService()

        # ë°°ì¹˜ ì²˜ë¦¬: 5ê°œì”© ë‚˜ëˆ„ì–´ì„œ ì²˜ë¦¬ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        batch_size = 5
        total_batches = (len(top_coins) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(top_coins))
            batch_coins = top_coins[start_idx:end_idx]

            logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì‹œì‘: {len(batch_coins)}ê°œ ì½”ì¸")

            # ë°°ì¹˜ ë‚´ ê° ì½”ì¸ë³„ ë¦¬ìŠ¤í¬ ë¶„ì„ ë° ì €ì¥
            for i, coin in enumerate(batch_coins):
                try:
                    logger.info(f"ğŸ” ë¦¬ìŠ¤í¬ ë¶„ì„ ì§„í–‰: {coin} ({start_idx + i + 1}/{len(top_coins)})")

                    # ë¹„ë™ê¸° ë¦¬ìŠ¤í¬ ë¶„ì„ ì‹¤í–‰
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

                    try:
                        # ê°œë³„ ì½”ì¸ë³„ ë³€ë™ì„±ê³¼ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë¦¬ìŠ¤í¬ ë¶„ì„
                        analysis_result = loop.run_until_complete(
                            analyze_individual_coin_risk(
                                coin, risk_service, days_back=90
                            )
                        )

                        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        loop.run_until_complete(
                            save_risk_analysis_to_database(analysis_result)
                        )

                        logger.info(f"âœ… {coin} ë¦¬ìŠ¤í¬ ë¶„ì„ ì™„ë£Œ")

                    finally:
                        loop.close()

                except Exception as e:
                    logger.error(f"âŒ {coin} ë¦¬ìŠ¤í¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    continue

            # ë°°ì¹˜ ê°„ ì ì‹œ ëŒ€ê¸° (API ì œí•œ ë°©ì§€)
            if batch_idx < total_batches - 1:
                logger.info("â³ ë‹¤ìŒ ë°°ì¹˜ ì²˜ë¦¬ ì „ 10ì´ˆ ëŒ€ê¸°...")
                time.sleep(10)

        logger.info("ğŸ‰ ë¦¬ìŠ¤í¬ ë¶„ì„ ìŠ¤ì¼€ì¤„ëŸ¬ ì™„ë£Œ")
        return {"status": "success", "analyzed_coins": len(top_coins)}

    except Exception as e:
        logger.error(f"âŒ ë¦¬ìŠ¤í¬ ë¶„ì„ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤íŒ¨: {str(e)}")
        raise

@celery_app.task(bind=True, name='scheduler.tasks.risk_analysis.get_all_risk_analyses')
def get_all_risk_analyses(self):
    """ì €ì¥ëœ ëª¨ë“  ë¦¬ìŠ¤í¬ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        import asyncpg

        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            conn = loop.run_until_complete(asyncpg.connect(
                host=database_config.POSTGRESQL_DB_HOST,
                port=int(database_config.POSTGRESQL_DB_PORT),
                database=database_config.POSTGRESQL_DB_DATABASE,
                user=database_config.POSTGRESQL_DB_USER,
                password=database_config.POSTGRESQL_DB_PASSWORD
            ))

            try:
                # ìµœê·¼ 24ì‹œê°„ ë°ì´í„° ì¡°íšŒ
                query = """
                    SELECT * FROM risk_analysis_reports
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    ORDER BY created_at DESC
                """

                rows = loop.run_until_complete(conn.fetch(query))

                results = []
                for row in rows:
                    result_dict = dict(row)
                    # JSONB í•„ë“œëŠ” ìë™ìœ¼ë¡œ íŒŒì‹±ë˜ë¯€ë¡œ ì¶”ê°€ ì²˜ë¦¬ ë¶ˆí•„ìš”
                    results.append(result_dict)

                logger.info(f"âœ… ë¦¬ìŠ¤í¬ ë¶„ì„ ì¡°íšŒ ì™„ë£Œ: {len(results)}ê°œ")
                return results

            finally:
                loop.run_until_complete(conn.close())

        finally:
            loop.close()

    except Exception as e:
        logger.error(f"âŒ ë¦¬ìŠ¤í¬ ë¶„ì„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise
